Downloaded enwiki-latest-pages-articles.xml.bz2 from http://meta.wikimedia.org/wiki/Data_dumps/
http://dumps.wikimedia.org/enwiki/ (2014-Dec-21 22:00:57) 

Then used http://medialab.di.unipi.it/wiki/Wikipedia_Extractor I changed one condition of the script so that links were preserved during the extraction
% bzcat enwiki-latest-pages-articles.xml.bz2 | python WikiExtractor.py -o extracted-with-links

Then I wrote a python script to get all the names of the wiki files the extractor produced
% python getFiles.py > wikiFileNamesPythonOutput.txt

Then I used java to get the first link from the list of files I generated
% javac FirstLinkFinder.java FirstLinkReader.java
% java FirstLinkFinder > linkPairList.txt

I use glogg to view the output file as it is a little over 4.4 million lines in length
I can't upload the output file to github due to its size

Then I made made another java program to sort the output into 27 files, one for each letter of the alphabet as well as an other for any numbers of other characters
% javac SortRaw.java 
% java SortRaw
This creates a directory called sortedOutPut and fills it with these 27 files
I beleive this will make it easier when searching for individual articles
