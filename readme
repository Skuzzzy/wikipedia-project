Downloaded enwiki-latest-pages-articles.xml.bz2 from http://meta.wikimedia.org/wiki/Data_dumps/
http://dumps.wikimedia.org/enwiki/ (2014-Dec-21 22:00:57) 

Then used http://medialab.di.unipi.it/wiki/Wikipedia_Extractor I changed one condition of the script so that links were preserved during the extraction
% bzcat enwiki-latest-pages-articles.xml.bz2 | python WikiExtractor.py -o extracted-with-links

Then I wrote a python script to get all the names of the wiki files the extractor produced
% python getFiles.py > wikiFileNamesPythonOutput.txt

Then I used java to get the first link from the list of files I generated
% javac FirstLinkFinder.java FirstLinkReader.java
% java FirstLinkFinder > linkPairList.txt

I use glogg to view the output file as it is a little over 4.4 million lines in length
I can't upload the output file to github due to its size

Then I made made another java program to sort the output into 27 files, one for each letter of the alphabet as well as an other for any numbers of other characters
% javac SortRaw.java 
% java SortRaw
This creates a directory called sortedOutPut and fills it with these 27 files
I beleive this will make it easier when searching for individual articles

Now for the more exciting part. I created Toy.java that takes 2 arguments, the number of articles you want to search through, as well as the intial article that you want to start the search at.
% javac Toy.java
% java Toy [NUM_ARTICLES_TO_CHECK] [INITIAL_ARTICLE]
This will recusively print the first link for each article unles it cannot find the next article listed
There is a large issue where I do not have a table of wikipedia redirects that the articles use which causes the search to fail for some links.
